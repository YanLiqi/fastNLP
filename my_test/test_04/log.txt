step:49: loss:8.43385
step:99: loss:8.43423
step:149: loss:8.41426
step:199: loss:8.34806
step:249: loss:8.32760
step:299: loss:8.32047
step:349: loss:8.31872
step:399: loss:8.31150
step:449: loss:8.31001
step:499: loss:8.30921
step:549: loss:8.30627
step:599: loss:8.30433
step:649: loss:8.30515
step:699: loss:8.30313
step:749: loss:8.30345
step:799: loss:8.30167
step:849: loss:8.30157
step:899: loss:8.29983
step:949: loss:8.29988
step:999: loss:8.29768
step:1049: loss:8.29778
step:1099: loss:8.30040
step:1149: loss:8.29545
step:1199: loss:8.29449
step:1249: loss:8.30015
step:1299: loss:8.31677
step:1349: loss:8.31103
step:1399: loss:8.30297
step:1449: loss:8.30370
step:1499: loss:8.30199
step:1549: loss:8.30080
step:1599: loss:8.29456
step:1649: loss:8.29851
step:1699: loss:8.29016
step:1749: loss:8.29712
step:1799: loss:8.29507
step:1849: loss:8.28967
step:1899: loss:8.28682
step:1949: loss:8.28413
step:1999: loss:8.29004
Evaluation at Epoch 1/50. Step:2022/101100. AccuracyMetric: acc=0.094796
step:2049: loss:8.28129
step:2099: loss:8.27933
step:2149: loss:8.28014
step:2199: loss:8.28314
step:2249: loss:8.28607
step:2299: loss:8.28248
step:2349: loss:8.28160
step:2399: loss:8.28750
step:2449: loss:8.28300
step:2499: loss:8.28461
step:2549: loss:8.28561
step:2599: loss:8.28360
step:2649: loss:8.28347
step:2699: loss:8.28127
step:2749: loss:8.28372
step:2799: loss:8.27837
step:2849: loss:8.28627
step:2899: loss:8.27899
step:2949: loss:8.27846
step:2999: loss:8.27583
step:3049: loss:8.27663
step:3099: loss:8.27538
step:3149: loss:8.27652
step:3199: loss:8.27696
step:3249: loss:8.27658
step:3299: loss:8.27419
step:3349: loss:8.27553
step:3399: loss:8.27590
step:3449: loss:8.28310
step:3499: loss:8.27748
step:3549: loss:8.27365
step:3599: loss:8.27553
step:3649: loss:8.27519
step:3699: loss:8.27436
step:3749: loss:8.27147
step:3799: loss:8.27244
step:3849: loss:8.27289
step:3899: loss:8.27852
step:3949: loss:8.27588
step:3999: loss:8.27456
Evaluation at Epoch 2/50. Step:4044/101100. AccuracyMetric: acc=0.097285
step:4049: loss:8.27500
step:4099: loss:8.27421
step:4149: loss:8.27417
step:4199: loss:8.27380
step:4249: loss:8.27251
step:4299: loss:8.26922
step:4349: loss:8.27332
step:4399: loss:8.27336
step:4449: loss:8.27108
step:4499: loss:8.27229
step:4549: loss:8.27295
step:4599: loss:8.27275
step:4649: loss:8.27486
step:4699: loss:8.27304
step:4749: loss:8.27207
step:4799: loss:8.27447
step:4849: loss:8.27406
step:4899: loss:8.27680
step:4949: loss:8.27231
step:4999: loss:8.27400
step:5049: loss:8.27054
step:5099: loss:8.26909
step:5149: loss:8.26962
step:5199: loss:8.27013
step:5249: loss:8.27380
step:5299: loss:8.27264
step:5349: loss:8.27509
step:5399: loss:8.27180
step:5449: loss:8.26919
step:5499: loss:8.26785
step:5549: loss:8.26760
step:5599: loss:8.26882
step:5649: loss:8.26924
step:5699: loss:8.26936
step:5749: loss:8.26762
step:5799: loss:8.27415
step:5849: loss:8.27066
step:5899: loss:8.27485
step:5949: loss:8.27313
step:5999: loss:8.26827
step:6049: loss:8.27388
Evaluation at Epoch 3/50. Step:6066/101100. AccuracyMetric: acc=0.100905
step:6099: loss:8.27303
step:6149: loss:8.26993
step:6199: loss:8.27006
step:6249: loss:8.26578
step:6299: loss:8.26716
step:6349: loss:8.26712
step:6399: loss:8.26503
step:6449: loss:8.26796
step:6499: loss:8.26829
step:6549: loss:8.26644
step:6599: loss:8.26798
step:6649: loss:8.27039
step:6699: loss:8.26567
step:6749: loss:8.26754
step:6799: loss:8.26393
step:6849: loss:8.26509
step:6899: loss:8.26838
step:6949: loss:8.26959
step:6999: loss:8.26351
step:7049: loss:8.26594
step:7099: loss:8.27113
step:7149: loss:8.27291
step:7199: loss:8.27524
step:7249: loss:8.27124
step:7299: loss:8.26555
step:7349: loss:8.26971
step:7399: loss:8.26984
step:7449: loss:8.26971
step:7499: loss:8.27096
step:7549: loss:8.27162
step:7599: loss:8.26769
step:7649: loss:8.26926
step:7699: loss:8.26715
step:7749: loss:8.26388
step:7799: loss:8.26462
step:7849: loss:8.26675
step:7899: loss:8.26915
step:7949: loss:8.26968
step:7999: loss:8.26576
step:8049: loss:8.26897
Evaluation at Epoch 4/50. Step:8088/101100. AccuracyMetric: acc=0.1
step:8099: loss:8.26830
step:8149: loss:8.26973
step:8199: loss:8.26487
step:8249: loss:8.26460
step:8299: loss:8.26347
step:8349: loss:8.26660
step:8399: loss:8.26153
step:8449: loss:8.26642
step:8499: loss:8.26908
step:8549: loss:8.26621
step:8599: loss:8.26776
step:8649: loss:8.26499
step:8699: loss:8.26398
step:8749: loss:8.26454
step:8799: loss:8.26782
step:8849: loss:8.26561
step:8899: loss:8.26596
step:8949: loss:8.26181
step:8999: loss:8.26287
step:9049: loss:8.26659
step:9099: loss:8.26159
step:9149: loss:8.26056
step:9199: loss:8.26413
step:9249: loss:8.26075
step:9299: loss:8.26506
step:9349: loss:8.26396
step:9399: loss:8.26750
step:9449: loss:8.26371
step:9499: loss:8.26456
step:9549: loss:8.26585
step:9599: loss:8.26489
step:9649: loss:8.26176
step:9699: loss:8.26632
step:9749: loss:8.26157
step:9799: loss:8.26222
step:9849: loss:8.26334
step:9899: loss:8.25919
step:9949: loss:8.26378
step:9999: loss:8.26192
step:10049: loss:8.26346
step:10099: loss:8.26193
Evaluation at Epoch 5/50. Step:10110/101100. AccuracyMetric: acc=0.102036
step:10149: loss:8.25916
step:10199: loss:8.26641
step:10249: loss:8.26413
step:10299: loss:8.26057
step:10349: loss:8.26322
step:10399: loss:8.26251
step:10449: loss:8.26144
step:10499: loss:8.25924
step:10549: loss:8.25731
step:10599: loss:8.26065
step:10649: loss:8.25989
step:10699: loss:8.26277
step:10749: loss:8.25987
step:10799: loss:8.26108
step:10849: loss:8.25651
step:10899: loss:8.25761
step:10949: loss:8.26261
step:10999: loss:8.25717
step:11049: loss:8.26646
step:11099: loss:8.25832
step:11149: loss:8.26417
step:11199: loss:8.26316
step:11249: loss:8.26403
step:11299: loss:8.25950
step:11349: loss:8.25935
step:11399: loss:8.25907
step:11449: loss:8.25941
step:11499: loss:8.26518
step:11549: loss:8.25999
step:11599: loss:8.26222
step:11649: loss:8.25691
step:11699: loss:8.25897
step:11749: loss:8.25988
step:11799: loss:8.26226
step:11849: loss:8.26218
step:11899: loss:8.26036
step:11949: loss:8.25972
step:11999: loss:8.26040
step:12049: loss:8.26020
step:12099: loss:8.25680
Evaluation at Epoch 6/50. Step:12132/101100. AccuracyMetric: acc=0.102262
step:12149: loss:8.26389
step:12199: loss:8.25895
step:12249: loss:8.25721
step:12299: loss:8.25772
step:12349: loss:8.25784
step:12399: loss:8.25614
step:12449: loss:8.25656
step:12499: loss:8.25574
step:12549: loss:8.25466
step:12599: loss:8.26110
step:12649: loss:8.25631
step:12699: loss:8.25815
step:12749: loss:8.25680
step:12799: loss:8.26039
step:12849: loss:8.25587
step:12899: loss:8.25891
step:12949: loss:8.25614
step:12999: loss:8.25898
step:13049: loss:8.25676
step:13099: loss:8.25794
step:13149: loss:8.25249
step:13199: loss:8.25766
step:13249: loss:8.26075
step:13299: loss:8.25621
step:13349: loss:8.25921
step:13399: loss:8.25875
step:13449: loss:8.25747
step:13499: loss:8.25120
step:13549: loss:8.25970
step:13599: loss:8.25633
step:13649: loss:8.25981
step:13699: loss:8.25882
step:13749: loss:8.26228
step:13799: loss:8.25758
step:13849: loss:8.25557
step:13899: loss:8.25578
step:13949: loss:8.25310
step:13999: loss:8.25478
step:14049: loss:8.25757
step:14099: loss:8.25583
step:14149: loss:8.25685
Evaluation at Epoch 7/50. Step:14154/101100. AccuracyMetric: acc=0.10543
step:14199: loss:8.25394
step:14249: loss:8.25020
step:14299: loss:8.25205
step:14349: loss:8.25687
step:14399: loss:8.25347
step:14449: loss:8.25520
step:14499: loss:8.25007
step:14549: loss:8.25577
step:14599: loss:8.25357
step:14649: loss:8.25474
step:14699: loss:8.25274
step:14749: loss:8.25380
step:14799: loss:8.25048
step:14849: loss:8.25397
step:14899: loss:8.25367
step:14949: loss:8.25220
step:14999: loss:8.25445
step:15049: loss:8.25359
step:15099: loss:8.25329
step:15149: loss:8.25492
step:15199: loss:8.25081
step:15249: loss:8.25579
step:15299: loss:8.25543
step:15349: loss:8.25445
step:15399: loss:8.25458
step:15449: loss:8.25191
step:15499: loss:8.25312
step:15549: loss:8.25119
step:15599: loss:8.25826
step:15649: loss:8.25246
step:15699: loss:8.25694
step:15749: loss:8.25455
step:15799: loss:8.24999
step:15849: loss:8.25391
step:15899: loss:8.25358
step:15949: loss:8.25518
step:15999: loss:8.25389
step:16049: loss:8.25427
step:16099: loss:8.25488
step:16149: loss:8.25255
Evaluation at Epoch 8/50. Step:16176/101100. AccuracyMetric: acc=0.106561
step:16199: loss:8.25334
step:16249: loss:8.24775
step:16299: loss:8.24834
step:16349: loss:8.25145
step:16399: loss:8.24971
step:16449: loss:8.25045
step:16499: loss:8.24988
step:16549: loss:8.25434
step:16599: loss:8.24935
step:16649: loss:8.25444
step:16699: loss:8.24909
step:16749: loss:8.25293
step:16799: loss:8.25003
step:16849: loss:8.24995
step:16899: loss:8.25116
step:16949: loss:8.25097
step:16999: loss:8.25355
step:17049: loss:8.25233
step:17099: loss:8.24868
step:17149: loss:8.25343
step:17199: loss:8.24949
step:17249: loss:8.24940
step:17299: loss:8.25219
step:17349: loss:8.25141
step:17399: loss:8.24536
step:17449: loss:8.25299
step:17499: loss:8.25172
step:17549: loss:8.25281
step:17599: loss:8.25362
step:17649: loss:8.25285
step:17699: loss:8.26336
step:17749: loss:8.25253
step:17799: loss:8.25138
step:17849: loss:8.25205
step:17899: loss:8.25731
step:17949: loss:8.25518
step:17999: loss:8.25296
step:18049: loss:8.25512
step:18099: loss:8.25050
step:18149: loss:8.25152
Evaluation at Epoch 9/50. Step:18198/101100. AccuracyMetric: acc=0.108597
step:18199: loss:8.25742
step:18249: loss:8.24922
step:18299: loss:8.25005
step:18349: loss:8.24810
step:18399: loss:8.24990
step:18449: loss:8.25539
step:18499: loss:8.25379
step:18549: loss:8.24976
step:18599: loss:8.24780
step:18649: loss:8.24544
step:18699: loss:8.24494
step:18749: loss:8.24938
step:18799: loss:8.25076
step:18849: loss:8.25105
step:18899: loss:8.25094
step:18949: loss:8.25348
step:18999: loss:8.25519
step:19049: loss:8.25411
step:19099: loss:8.25383
step:19149: loss:8.25322
step:19199: loss:8.25825
step:19249: loss:8.25271
step:19299: loss:8.25392
step:19349: loss:8.25115
step:19399: loss:8.24985
step:19449: loss:8.25457
step:19499: loss:8.25337
step:19549: loss:8.25145
step:19599: loss:8.24937
step:19649: loss:8.25116
step:19699: loss:8.25486
step:19749: loss:8.24821
step:19799: loss:8.24885
step:19849: loss:8.25400
step:19899: loss:8.25690
step:19949: loss:8.25288
step:19999: loss:8.25015
step:20049: loss:8.25047
step:20099: loss:8.24921
step:20149: loss:8.25033
step:20199: loss:8.25162
Evaluation at Epoch 10/50. Step:20220/101100. AccuracyMetric: acc=0.109276
step:20249: loss:8.24642
step:20299: loss:8.24988
step:20349: loss:8.25362
step:20399: loss:8.25016
step:20449: loss:8.25229
step:20499: loss:8.25231
step:20549: loss:8.25112
step:20599: loss:8.25461
step:20649: loss:8.25640
step:20699: loss:8.25382
step:20749: loss:8.25192
step:20799: loss:8.25242
step:20849: loss:8.25072
step:20899: loss:8.25088
step:20949: loss:8.25063
step:20999: loss:8.25142
step:21049: loss:8.25404
step:21099: loss:8.24866
step:21149: loss:8.25052
step:21199: loss:8.24900
step:21249: loss:8.25542
step:21299: loss:8.24725
step:21349: loss:8.25757
step:21399: loss:8.25369
step:21449: loss:8.25027
step:21499: loss:8.25037
step:21549: loss:8.25072
step:21599: loss:8.25117
step:21649: loss:8.25351
step:21699: loss:8.24176
step:21749: loss:8.24933
step:21799: loss:8.24926
step:21849: loss:8.24996
step:21899: loss:8.25237
step:21949: loss:8.24431
step:21999: loss:8.24920
step:22049: loss:8.25051
step:22099: loss:8.24493
step:22149: loss:8.24806
step:22199: loss:8.25017
Evaluation at Epoch 11/50. Step:22242/101100. AccuracyMetric: acc=0.105204
step:22249: loss:8.25260
step:22299: loss:8.24726
step:22349: loss:8.24667
step:22399: loss:8.24481
step:22449: loss:8.24444
step:22499: loss:8.24323
step:22549: loss:8.24442
step:22599: loss:8.24808
step:22649: loss:8.24492
step:22699: loss:8.24651
step:22749: loss:8.24892
step:22799: loss:8.24784
step:22849: loss:8.24876
step:22899: loss:8.24846
step:22949: loss:8.24403
step:22999: loss:8.24542
step:23049: loss:8.24803
step:23099: loss:8.25063
step:23149: loss:8.24386
step:23199: loss:8.24901
step:23249: loss:8.24717
step:23299: loss:8.24863
step:23349: loss:8.24774
step:23399: loss:8.24194
step:23449: loss:8.24877
step:23499: loss:8.24569
step:23549: loss:8.24352
step:23599: loss:8.24825
step:23649: loss:8.24757
step:23699: loss:8.24437
step:23749: loss:8.24293
step:23799: loss:8.24588
step:23849: loss:8.25262
step:23899: loss:8.24964
step:23949: loss:8.25034
step:23999: loss:8.24775
step:24049: loss:8.24478
step:24099: loss:8.24581
step:24149: loss:8.24531
step:24199: loss:8.24780
step:24249: loss:8.24613
Evaluation at Epoch 12/50. Step:24264/101100. AccuracyMetric: acc=0.106561
step:24299: loss:8.24552
step:24349: loss:8.24570
step:24399: loss:8.24062
step:24449: loss:8.24095
step:24499: loss:8.24176
step:24549: loss:8.24195
step:24599: loss:8.24258
step:24649: loss:8.24146
step:24699: loss:8.24377
step:24749: loss:8.24242
step:24799: loss:8.24944
step:24849: loss:8.24558
step:24899: loss:8.24719
step:24949: loss:8.24899
step:24999: loss:8.24507
step:25049: loss:8.23966
step:25099: loss:8.24381
step:25149: loss:8.24321
step:25199: loss:8.24866
step:25249: loss:8.24395
step:25299: loss:8.24700
step:25349: loss:8.24270
step:25399: loss:8.24272
step:25449: loss:8.24301
step:25499: loss:8.24270
step:25549: loss:8.24468
step:25599: loss:8.25047
step:25649: loss:8.24747
step:25699: loss:8.24560
step:25749: loss:8.24178
step:25799: loss:8.24789
step:25849: loss:8.24224
step:25899: loss:8.24157
step:25949: loss:8.24294
step:25999: loss:8.24586
step:26049: loss:8.24763
step:26099: loss:8.24912
step:26149: loss:8.24538
step:26199: loss:8.24605
step:26249: loss:8.24364
Evaluation at Epoch 13/50. Step:26286/101100. AccuracyMetric: acc=0.108371
step:26299: loss:8.24468
step:26349: loss:8.24590
step:26399: loss:8.24139
step:26449: loss:8.24234
step:26499: loss:8.23988
step:26549: loss:8.24173
step:26599: loss:8.24285
step:26649: loss:8.24100
step:26699: loss:8.24135
step:26749: loss:8.24258
step:26799: loss:8.24162
step:26849: loss:8.24030
step:26899: loss:8.23910
step:26949: loss:8.24536
step:26999: loss:8.24265
step:27049: loss:8.24176
step:27099: loss:8.24437
step:27149: loss:8.23657
step:27199: loss:8.24246
step:27249: loss:8.24632
step:27299: loss:8.24060
step:27349: loss:8.24299
step:27399: loss:8.24373
step:27449: loss:8.24230
step:27499: loss:8.24495
step:27549: loss:8.24615
step:27599: loss:8.23836
step:27649: loss:8.24210
step:27699: loss:8.24271
step:27749: loss:8.24140
step:27799: loss:8.24003
step:27849: loss:8.24328
step:27899: loss:8.24335
step:27949: loss:8.24179
step:27999: loss:8.24251
step:28049: loss:8.24420
step:28099: loss:8.24197
step:28149: loss:8.24259
step:28199: loss:8.24292
step:28249: loss:8.24178
step:28299: loss:8.24369
Evaluation at Epoch 14/50. Step:28308/101100. AccuracyMetric: acc=0.108371
step:28349: loss:8.23768
step:28399: loss:8.24182
step:28449: loss:8.23605
step:28499: loss:8.24006
step:28549: loss:8.24027
step:28599: loss:8.23945
step:28649: loss:8.24256
step:28699: loss:8.23948
step:28749: loss:8.24012
step:28799: loss:8.23732
step:28849: loss:8.23964
step:28899: loss:8.23662
step:28949: loss:8.23628
step:28999: loss:8.24277
step:29049: loss:8.23954
step:29099: loss:8.23301
step:29149: loss:8.24273
step:29199: loss:8.23618
step:29249: loss:8.24173
step:29299: loss:8.23795
step:29349: loss:8.24096
step:29399: loss:8.23753
step:29449: loss:8.23670
step:29499: loss:8.23945
step:29549: loss:8.24100
step:29599: loss:8.24134
step:29649: loss:8.23778
step:29699: loss:8.23695
step:29749: loss:8.24510
step:29799: loss:8.24246
step:29849: loss:8.24107
step:29899: loss:8.23830
step:29949: loss:8.23926
step:29999: loss:8.24469
step:30049: loss:8.24065
step:30099: loss:8.24047
step:30149: loss:8.24318
step:30199: loss:8.23718
step:30249: loss:8.23667
step:30299: loss:8.23776
Evaluation at Epoch 15/50. Step:30330/101100. AccuracyMetric: acc=0.112896
step:30349: loss:8.23930
step:30399: loss:8.24081
step:30449: loss:8.23693
step:30499: loss:8.24039
step:30549: loss:8.23748
step:30599: loss:8.23098
step:30649: loss:8.23561
step:30699: loss:8.23937
step:30749: loss:8.23856
step:30799: loss:8.23815
step:30849: loss:8.23703
step:30899: loss:8.23864
step:30949: loss:8.23937
step:30999: loss:8.23977
step:31049: loss:8.23498
step:31099: loss:8.23709
step:31149: loss:8.24109
step:31199: loss:8.23686
step:31249: loss:8.23928
step:31299: loss:8.23949
step:31349: loss:8.23733
step:31399: loss:8.23859
step:31449: loss:8.23557
